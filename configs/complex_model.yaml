# An example configuration for a more complex network.


model:
  name: complex_model
  nodes:
    - name: EfficientRep

    - name: RepPANNeck
      inputs:
        - EfficientRep

    - name: EfficientKeypointBBoxHead
      inputs:
        - RepPANNeck

      losses:
        - name: EfficientKeypointBBoxLoss

      metrics:
        - name: ObjectKeypointSimilarity
          is_main_metric: true
        - name: MeanAveragePrecision

      visualizers:
        - name: KeypointVisualizer
          params:
            nonvisible_color: blue

    - name: SegmentationHead
      inputs:
        - RepPANNeck
      losses:
        - name: BCEWithLogitsLoss
      metrics:
        - name: F1Score
          params:
            task: binary
        - name: JaccardIndex
          params:
            task: binary
      visualizers:
        - name: SegmentationVisualizer
          params:
            colors: "#FF5055"

    - name: EfficientBBoxHead
      inputs:
        - RepPANNeck
      params:
        conf_thres: 0.75
        iou_thres: 0.45
      losses:
        - name: AdaptiveDetectionLoss
      metrics:
        - name: MeanAveragePrecision
      visualizers:
        - name: BBoxVisualizer

tracker:
  project_name: coco_test
  save_directory: output
  is_tensorboard: true
  is_wandb: false
  is_mlflow: false

loader:
  train_view: train
  val_view: val
  test_view: test

  params:
    dataset_name: coco_test

trainer:
  accelerator: auto
  devices: auto
  strategy: auto

  n_sanity_val_steps: 1
  batch_size: 8
  accumulate_grad_batches: 1
  epochs: 200
  n_workers: 8
  validation_interval: 10
  n_log_images: 8
  skip_last_batch: true
  log_sub_losses: true
  save_top_k: 3

  preprocessing:
    train_image_size: [384, 384]
    keep_aspect_ratio: true
    normalize:
      active: true
    augmentations:
      - name: Defocus
        params:
          p: 0.1
      - name: Sharpen
        params:
          p: 0.1
      - name: HorizontalFlip
      - name: RandomRotate90
      - name: Mosaic4

  callbacks:
    - name: LearningRateMonitor
      params:
        logging_interval: step
    - name: MetadataLogger
      params:
        hyperparams: ["trainer.epochs", trainer.batch_size]
    - name: EarlyStopping
      params:
        patience: 3
        monitor: val/loss
        mode: min
    - name: ExportOnTrainEnd
    - name: ArchiveOnTrainEnd
    - name: TestOnTrainEnd

  optimizer:
    name: SGD
    params:
      lr: 0.02
      momentum: 0.937
      nesterov: true
      weight_decay: 0.0005

  scheduler:
    name: CosineAnnealingLR
    params:
      eta_min: 0

exporter:
  onnx:
    opset_version: 11

tuner:
  params:
    trainer.optimizer.name_categorical: ["Adam", "SGD"]
    trainer.optimizer.params.lr_float: [0.0001, 0.001]
    trainer.batch_size_int: [4, 16, 4]
