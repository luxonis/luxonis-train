# An example configuration for a more complex network.


model:
  name: coco_test
  nodes:
    - name: EfficientRep
      variant: "n"

    - name: RepPANNeck
      inputs:
        - EfficientRep

    - name: ImplicitKeypointBBoxHead
      inputs:
        - RepPANNeck
      params:
        conf_thres: 0.25
        iou_thres: 0.45
      losses:
        name: ImplicitKeypointBBoxLoss
        params:
          keypoint_regression_loss_weight: 0.5
          keypoint_visibility_loss_weight: 0.7
          bbox_loss_weight: 0.05
          objectness_loss_weight: 0.2
      metrics:
        - name: ObjectKeypointSimilarity
          is_main_metric: true
        - name: MeanAveragePrecisionKeypoints

      visualizers:
        name: MultiVisualizer
        attached_to: ImplicitKeypointBBoxHead
        params:
          visualizers:
            - name: KeypointVisualizer
              params:
                nonvisible_color: blue
            - name: BBoxVisualizer
              params:
                colors:
                  person: "#FF5055"

    - name: SegmentationHead
      inputs:
        - RepPANNeck
      losses:
        name: BCEWithLogitsLoss
      metrics:
        - name: F1Score
          params:
            task: binary
        - name: JaccardIndex
          params:
            task: binary
      visualizers:
        name: SegmentationVisualizer
        params:
          colors: "#FF5055"

    - name: EfficientBBoxHead
      inputs:
        - RepPANNeck
      params:
        conf_thres: 0.75
        iou_thres: 0.45
      losses:
        name: AdaptiveDetectionLoss
      metrics:
        name: MeanAveragePrecision
      visualizers:
        name: BBoxVisualizer

tracker:
  project_name: coco_test
  save_directory: output
  is_tensorboard: True
  is_wandb: False
  wandb_entity: luxonis
  is_mlflow: False

loader:
  train_view: train
  val_view: val
  test_view: test

  params:
    dataset_name: coco_test

trainer:
  accelerator: auto
  devices: auto
  strategy: auto

  n_sanity_val_steps: 1
  profiler: null
  verbose: True
  batch_size: 8
  accumulate_grad_batches: 1
  epochs: &epochs 200
  n_workers: 8
  train_metrics_interval: -1
  validation_interval: 10
  n_log_images: 8
  skip_last_batch: True
  log_sub_losses: True
  save_top_k: 3

  preprocessing:
    train_image_size: [&height 384, &width 512]
    keep_aspect_ratio: True
    train_rgb: True
    normalize:
      active: True
    augmentations:
      - name: Defocus
        params:
          p: 0.1
      - name: Sharpen
        params:
          p: 0.1
      - name: Flip
      - name: RandomRotate90
      - name: Mosaic4
        params:
          out_width: *width
          out_height: *height

  callbacks:
    - name: LearningRateMonitor
      params:
        logging_interval: step
    - name: MetadataLogger
      params:
        hyperparams: ["trainer.epochs", trainer.batch_size]
    - name: EarlyStopping
      params:
        patience: 3
        monitor: val/loss
        mode: min
        verbose: true
    - name: ExportOnTrainEnd
    - name: TestOnTrainEnd

  optimizer:
    name: SGD
    params:
      lr: 0.02
      momentum: 0.937
      nesterov: True
      weight_decay: 0.0005

  scheduler:
    name: CosineAnnealingLR
    params:
      T_max: *epochs
      eta_min: 0

exporter:
  onnx:
    opset_version: 11

tuner:
  params:
    trainer.optimizer.name_categorical: ["Adam", "SGD"]
    trainer.optimizer.params.lr_float: [0.0001, 0.001]
    trainer.batch_size_int: [4, 16, 4]
